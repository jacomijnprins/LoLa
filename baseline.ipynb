{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOozjqUGlZ4m/S/RI9qXXtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacomijnprins/LoLa/blob/Jessica/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "foeu3qRIi9gZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snli = load_dataset(\"snli\")"
      ],
      "metadata": {
        "id": "3XodDOL6jdyd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREP DATA FOR HYPERPARAMETER TRAINING\n",
        "# Filter to remove invalid rows (-1 labels)\n",
        "for split in snli:\n",
        "    snli[split] = snli[split].filter(lambda x: x[\"label\"] >= 0)\n",
        "\n",
        "# Specify the split and sample size\n",
        "split = \"train\"\n",
        "sample_size = 1000\n",
        "\n",
        "# Ensure reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Randomly sample three subsets\n",
        "subset1 = snli[split].shuffle(seed=42).select(range(sample_size))\n",
        "subset2 = snli[split].shuffle(seed=43).select(range(sample_size))\n",
        "subset3 = snli[split].shuffle(seed=44).select(range(sample_size))\n",
        "subset4 = snli[split].shuffle(seed=45).select(range(sample_size))\n",
        "subset5 = snli[split].shuffle(seed=46).select(range(sample_size))\n",
        "\n",
        "\n",
        "\n",
        "# Print a summary of the subsets\n",
        "print(\"Subset 1:\", subset1)\n",
        "print(\"Subset 2:\", subset2)\n",
        "print(\"Subset 3:\", subset3)\n",
        "print(\"Subset 4:\", subset4)\n",
        "print(\"Subset 5:\", subset5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc81hxTcjvzO",
        "outputId": "b1eb1dc9-6fd3-4c9d-939d-2134e330ae52"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset 1: Dataset({\n",
            "    features: ['premise', 'hypothesis', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Subset 2: Dataset({\n",
            "    features: ['premise', 'hypothesis', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Subset 3: Dataset({\n",
            "    features: ['premise', 'hypothesis', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Subset 4: Dataset({\n",
            "    features: ['premise', 'hypothesis', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Subset 5: Dataset({\n",
            "    features: ['premise', 'hypothesis', 'label'],\n",
            "    num_rows: 1000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsets = [subset1, subset2, subset3, subset4, subset5]"
      ],
      "metadata": {
        "id": "3JMJzjSunpvL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(dataset, train_ratio=0.8):\n",
        "  train_size = int(train_ratio*len(dataset))\n",
        "\n",
        "  train_dataset = dataset.select(range(train_size))\n",
        "  val_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "\n",
        "  return train_dataset, val_dataset\n",
        "\n",
        "#split subsets into training and validation and encode data\n",
        "finetune_data = []\n",
        "for subset in subsets:\n",
        "  train_dataset, val_dataset = train_val_split(subset)\n",
        "  train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "  val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "  finetune_data.append((train_dataset, val_dataset))\n"
      ],
      "metadata": {
        "id": "CHzoijq7ozMK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aqJhn7jDqgJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## PREP MODEL\n",
        "\n",
        "#Load the tokenizer and model\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "# Tokenize the dataset\n",
        "#encoded_snli = snli.map(tokenize_function, batched=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xia3VJOmKaw",
        "outputId": "11c99531-a5e3-451d-a29d-d337ce0f3c7a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
      ],
      "metadata": {
        "id": "vUtacmGwpwOt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning Distilbert learning_rate=2e-5, decay=00.1\n",
        "train1, val1 = finetune_data[0]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",)\n",
        "\n",
        "trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train1,\n",
        "      eval_dataset=val1,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "AEYhbeIooZS4",
        "outputId": "86042970-b2d4-4eb0-f0c6-09e11eb21ef6"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-54-96beab147fe8>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.091713</td>\n",
              "      <td>0.405000</td>\n",
              "      <td>0.275375</td>\n",
              "      <td>0.355612</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=1.0999875640869141, metrics={'train_runtime': 276.1812, 'train_samples_per_second': 2.897, 'train_steps_per_second': 0.181, 'total_flos': 8672629667040.0, 'train_loss': 1.0999875640869141, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning Distilbert learning_rate=2e-5, decay=00.1\n",
        "train2, val2 = finetune_data[1]\n",
        "\n",
        "\n",
        "training_args2 = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",)\n",
        "\n",
        "trainer2 = Trainer(\n",
        "      model=model,\n",
        "      args=training_args2,\n",
        "      train_dataset=train2,\n",
        "      eval_dataset=val2,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "trainer2.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "VC5U14fUqvod",
        "outputId": "7594b83f-e49c-450f-aeff-71597976d0e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-56-05129d01bb41>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer2 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.047704</td>\n",
              "      <td>0.485000</td>\n",
              "      <td>0.421677</td>\n",
              "      <td>0.467894</td>\n",
              "      <td>0.485000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=1.0414228057861328, metrics={'train_runtime': 271.3423, 'train_samples_per_second': 2.948, 'train_steps_per_second': 0.184, 'total_flos': 8747143907616.0, 'train_loss': 1.0414228057861328, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning Distilbert learning_rate=2e-5, decay=00.1\n",
        "train3, val3 = finetune_data[2]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.001,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",)\n",
        "\n",
        "trainer3 = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train3,\n",
        "      eval_dataset=val3,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "trainer3.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "oRMaxAbW6Oxo",
        "outputId": "7ff308e5-9909-411e-cbe6-2e37fcc83249"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-58-56e29b6cf416>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer3 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.005953</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>0.455162</td>\n",
              "      <td>0.593087</td>\n",
              "      <td>0.520000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=1.031500930786133, metrics={'train_runtime': 258.883, 'train_samples_per_second': 3.09, 'train_steps_per_second': 0.193, 'total_flos': 8668489987008.0, 'train_loss': 1.031500930786133, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train4, val4 = finetune_data[3]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.00001,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",)\n",
        "\n",
        "trainer4 = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train4,\n",
        "      eval_dataset=val4,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "trainer4.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "7t4yGct48yrM",
        "outputId": "0112d215-3323-42e4-c9ee-7853fcfb1791"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-60-a90edc4315d2>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer4 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:32, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.967971</td>\n",
              "      <td>0.535000</td>\n",
              "      <td>0.494930</td>\n",
              "      <td>0.547454</td>\n",
              "      <td>0.535000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.9777275848388672, metrics={'train_runtime': 281.5979, 'train_samples_per_second': 2.841, 'train_steps_per_second': 0.178, 'total_flos': 8788540707936.0, 'train_loss': 0.9777275848388672, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train5, val5 = finetune_data[4]\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.00001,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",)\n",
        "\n",
        "trainer5 = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train5,\n",
        "      eval_dataset=val5,\n",
        "      tokenizer=tokenizer,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "trainer5.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "68VptSRu-ais",
        "outputId": "b951eff9-a702-45e5-ccf5-768a1aa9bd8e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-61-7f4bcf6606ab>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer5 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 04:18, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.922357</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>0.549677</td>\n",
              "      <td>0.573453</td>\n",
              "      <td>0.575000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.9356491088867187, metrics={'train_runtime': 262.9493, 'train_samples_per_second': 3.042, 'train_steps_per_second': 0.19, 'total_flos': 8883753348672.0, 'train_loss': 0.9356491088867187, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}