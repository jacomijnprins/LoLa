{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model on cartography classes\n",
    "\n",
    "I want to see whether selecting easy-to-learn, hard-to-learn or ambiguous data instances will omprove performance on our 2k curriculum as well.\n",
    "Might make multiple curriculum with:\n",
    "- an equal mix of the three groups\n",
    "- just ambiguous \n",
    "- just hard-to-learn\n",
    "\n",
    "Paper: Swayamdipta, et all. (2020). Dataset cartography: Mapping and diagnosing datasets with training dynamics. arXiv preprint arXiv:2009.10795."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets\n",
    "### Importing cartography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cartography_data = pd.read_json('snli_roberta_0_6_data_map_coordinates.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367\n",
      "                 guid   index  confidence  variability  correctness\n",
      "0       4446013212032       0    0.675678     0.301201            4\n",
      "1        568940932410       1    0.843770     0.314807            5\n",
      "2       4107496324431       2    0.864639     0.302112            5\n",
      "3        151853983012       3    0.872000     0.154454            6\n",
      "4       6041167176312       4    0.653921     0.342495            4\n",
      "...               ...     ...         ...          ...          ...\n",
      "549362  5732659631411  549362    0.997563     0.002784            6\n",
      "549363  3920105265011  549363    0.994982     0.010534            6\n",
      "549364  6888801884010  549364    0.936412     0.080370            6\n",
      "549365   189022647122  549365    0.777098     0.066182            6\n",
      "549366  4506417051031  549366    0.913720     0.081541            6\n",
      "\n",
      "[549367 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(len(cartography_data))\n",
    "print(cartography_data) # 4804607632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing SNLI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if assigntools not yet downloaded run line\n",
    "# ! git clone https://github.com/kovvalsky/assigntools.git\n",
    "\n",
    "# if zip file of SNLI data not yet downloaded run lines\n",
    "# !wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
    "# !unzip snli_1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550152, 10)\n"
     ]
    }
   ],
   "source": [
    "# original dataset from Stanford\n",
    "data_snli = pd.read_json('snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
    "# data.columns = [\"guid\", \"index\", \"confidence\", \"variability\", \"correctness\"]\n",
    "# print(data_snli[\"annotator_labels\"])\n",
    "print(data_snli.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assigntools.LoLa.read_nli import snli_jsonl2dict, sen2anno_from_nli_problems\n",
    "from assigntools.LoLa.sen_analysis import spacy_process_sen2tok, display_doc_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .json files for ['dev', 'test', 'train'] parts\n",
      "processing DEV:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 22340.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9842 problems read\n",
      "0 problems have a wrong annotator label\n",
      "processing TEST:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 20434.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9824 problems read\n",
      "0 problems have a wrong annotator label\n",
      "processing TRAIN:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550152it [00:26, 20683.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549169 problems read\n",
      "198 problems have a wrong annotator label\n",
      "Most common weird labels: //(198)\n",
      "Length of the SNLI dataset with the wrong labels: 549169\n"
     ]
    }
   ],
   "source": [
    "# this is Lasha's code for downloading SNLI\n",
    "SNLI, S2A = snli_jsonl2dict('snli_1.0') \n",
    "print(f\"Length of the SNLI dataset with the wrong labels: {len(SNLI_with_wrong['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .json files for ['dev', 'test', 'train'] parts\n",
      "processing DEV:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:00, 19459.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9842 problems read\n",
      "0 problems have a wrong annotator label\n",
      "processing TEST:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:04, 2444.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9824 problems read\n",
      "0 problems have a wrong annotator label\n",
      "processing TRAIN:\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "550152it [00:26, 20979.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549367 problems read\n",
      "198 problems have a wrong annotator label\n",
      "Most common weird labels: //(198)\n",
      "Length of the clean SNLI dataset: 549169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# this is Lasha's code for downloading SNLI without cleaning the labels\n",
    "SNLI_with_wrong, S2A = snli_jsonl2dict('snli_1.0', clean_labels=False)\n",
    "print(f\"Length of the clean SNLI dataset: {len(SNLI['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to find a way to match up the number of instances of the cartography data to the number of instances in the original SNLI dataset in order to match the ID's from the cartography values to the sentences to train the model on.\n",
    "- The cartography dataset has *549367*\n",
    "- The original SNLI dataset downloaded from Stanford or huggingface has *550152*\n",
    "- From Lasha's code has *549169* with 198 with wrong annotater labels\n",
    "\n",
    "This suggests that if we delete the instances with wrong annotated labels from the cartography dataset that the instances from the cartography dataset would match that of Lasha's processed dataset and we could match up the ID's of the two datasets.\n",
    "\n",
    "Task:\n",
    "- Check if the ID's of the two datasets (SNLI and cartography) match up. Can use the code in which they assign the ID's to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "# print(x for x in list(SNLI['train'].keys()) if x not in list(SNLI_with_wrong['train'].keys()))\n",
    "wrong_keys = list(set(list(SNLI_with_wrong['train'].keys())).difference(list(SNLI['train'].keys())))\n",
    "print(len(wrong_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to be able to delete the wrong label ID's from the cartography dataset.\\\n",
    "For that first see how to link the ID's from the SNLI dataset to that of the cartography data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils_glue import convert_string_to_unique_number\n",
    "\n",
    "print(number_ID = convert_string_to_unique_number(wrong_keys[0])) #5696581092010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5696581092010, 375392855412, 371769150011, 4850614476111, 562928217311, 436608339411, 3228960484012, 200278000411, 246901891011, 3309042087411, 299193125211, 5716554915110, 4815080638312, 4045361947410, 300577374112, 2166946777412, 4482946929010, 888244173410, 4313861353310, 2862004252410, 4123816289310, 407278057211, 4792134256012, 4070658400011, 7616312438011, 2966552760211, 4622296311110, 1932314876412, 3532539748312, 2324374253012, 2915792034011, 5867606212, 4704939941012, 1752454466212, 449764037011, 2279380309211, 3670205710011, 418616992310, 4450153946210, 735787579112, 250699226210, 3423249426011, 470903027410, 434932657111, 4701385015011, 3650485497112, 3945002600010, 421762501012, 4418471031111, 4357061908311, 2894893895012, 2537692668112, 5903528077412, 4546867536111, 3758175529011, 6948564341412, 7803420092212, 158898445210, 4662376933112, 3247385464011, 7237669608111, 4587222385110, 5532294954011, 5117916560111, 4684510937312, 5609573810011, 4625272395210, 4552688825412, 6887015851111, 3153720471111, 3189327551010, 284251489311, 1358089136011, 239453674011, 2470318576010, 3370308329010, 2331094501010, 4799726314411, 2677565007112, 4761917952410, 4487186202411, 513382879311, 2688731661111, 301315162011, 3033257301012, 1197800988312, 6727599793211, 2506460104212, 8037906430310, 7161625352011, 2713897716411, 4652704643110, 6141300695211, 3005393426011, 2887614578311, 2847514745410, 3523874798410, 7057696291110, 4872410849210, 267164457312, 2585141045111, 4834121945410, 6082212650112, 8111420340411, 4970865205412, 621668012010, 4689844217022, 4879964792111, 4701385015012, 395125320411, 582899605410, 1356543628411, 7065455241010, 353180303211, 1935109593010, 2580503256010, 5403970454012, 2473737724210, 2504277798411, 3241531692011, 4689844217020, 3826412077211, 28753061012, 5468293105122, 212857743211, 4668422696211, 2888517039411, 2391048897411, 3067824182311, 4950818767211, 2383712490411, 3596356507311, 7355491286012, 3497565955010, 3666179842110, 561204294011, 5633423671111, 3705470430010, 2508843920011, 6301391567312, 4559148111311, 4841630077310, 843616798111, 1974336555012, 2756591658011, 4871416563412, 508261758410, 2346709709011, 4532084687111, 2469827608010, 6850198746010, 6685838035312, 1414779054012, 7987908185012, 3413806271212, 5716219952410, 6647909243211, 511186815011, 55034909210, 4732642723112, 4787520787410, 2517553414012, 83191345010, 2882056260412, 3266261886410, 3381747300010, 3208271313110, 4523579347210, 4379576888112, 7006400639011, 6218174603011, 7979221902011, 3014015906311, 4830620136412, 3673165148411, 724530150312, 21167868011, 4809880189211, 7914606438011, 3042173467212, 4600972102011, 2494088238210, 4269551560411, 7281664064212, 3328324395010, 4308849697311, 2206282062110, 3760799822111, 7558549616012, 65298461011, 2812125355411, 4824883554411, 467864644310, 420355149112, 3498388886010, 395125320410, 386640177112, 4663966489312]\n"
     ]
    }
   ],
   "source": [
    "# the guid values for the data instances that have a bad gold label\n",
    "wrong_indeces = []\n",
    "for key in wrong_keys:\n",
    "    wrong_indeces.append(convert_string_to_unique_number(key))\n",
    "print(wrong_indeces)\n",
    "\n",
    "# print(list(cartography_data['guid']).index(number_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549169\n"
     ]
    }
   ],
   "source": [
    "# deleting the wrong indeces from the cartography dataset\n",
    "for ID in wrong_indeces:\n",
    "    cartography_data = cartography_data[cartography_data['guid'] != ID]\n",
    "\n",
    "print(len(cartography_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect the right data instance from the cartography dataset to the right instance from the SNLI dataset I'm adding the cartography guid to the SNLI dataset to make it an easier lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g': 'neutral',\n",
       " 'pid': '3416050480.jpg#4r1n',\n",
       " 'cid': '3416050480.jpg#4',\n",
       " 'lnum': 1,\n",
       " 'lcnt': Counter({'neutral': 1}),\n",
       " 'ltype': '010',\n",
       " 'p': 'A person on a horse jumps over a broken down airplane.',\n",
       " 'h': 'A person is training his horse for a competition.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(SNLI['train'][list(SNLI['train'].keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'g': 'neutral',\n",
       " 'pid': '3416050480.jpg#4r1n',\n",
       " 'cid': '3416050480.jpg#4',\n",
       " 'lnum': 1,\n",
       " 'lcnt': Counter({'neutral': 1}),\n",
       " 'ltype': '010',\n",
       " 'p': 'A person on a horse jumps over a broken down airplane.',\n",
       " 'h': 'A person is training his horse for a competition.',\n",
       " 'guid': 3416050480412}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key in SNLI['train'].keys():\n",
    "    SNLI['train'][key]['guid'] = convert_string_to_unique_number(key)\n",
    "\n",
    "display(SNLI['train'][list(SNLI['train'].keys())[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the curicula\n",
    "I want to use the different groups (hard-to-learn, easy-to-learn, ambiguous) based on the values in the cartography dataset. Then try the same training sets (just hard-to-learn and just ambiguous) as used in the paper and, if times allows, also a mix. This will allow us to answer the question can we find the same patterns as the paper in our curriculums of 2k? \n",
    "\n",
    "Tasks:\n",
    "- Look into what the values are that they used in the paper to destinguish the three groups. Paper doesn't mention values --> look at code\n",
    "- How to take 2k datapoints from those bins. Do you want to take those randomly or make smaller bins inside of the groups to select a good sample of that particular group?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
